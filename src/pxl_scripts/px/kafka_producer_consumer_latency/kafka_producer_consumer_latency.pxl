# Copyright 2021- The Pixie Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

''' Kafka Producer-Consumer Latency

This script measures the latency for a Kafka producer-consumer pair.
Limitations: only works if producer/consumer operate on a single topic.
'''
import px


def kafka_producers(start_time: str):

    df = px.DataFrame(table='kafka_events.beta', start_time=start_time)
    df = add_source_dest_columns(df)
    df = df.groupby(['source', 'client_id', 'req_cmd', 'namespace']).agg()
    df = df[df.req_cmd == 0]
    df.producer = df.client_id
    return df[['producer', 'namespace', 'source']]


def kafka_consumers(start_time: str):

    df = px.DataFrame(table='kafka_events.beta', start_time=start_time)
    df = add_source_dest_columns(df)
    df = df.groupby(['source', 'client_id', 'req_cmd', 'namespace']).agg()
    df = df[df.req_cmd == 1]
    df.consumer = df.client_id
    return df[['consumer', 'namespace', 'source']]


def kafka_data(start_time: str, producer: str, consumer: str, topic: str):
    df = px.DataFrame(table='kafka_events.beta', start_time=start_time)

    df.namespace = df.ctx['namespace']
    df.node = df.ctx['node']
    df.pod = df.ctx['pod']
    df.pid = px.upid_to_pid(df.upid)

    # Filter on the requests to the kafka topic.
    # This needs to be made more robust once PxL has better JSON querying support.
    topic_str = '"name":"' + topic + '"'
    df = df[px.contains(df.req_body, topic_str) or px.contains(df.resp, topic_str)]

    # Filter to consumer-producer pair of interest.
    consumer_df = df[df.client_id == consumer]
    producer_df = df[df.client_id == producer]

    # Extract producer and consumer offsets.
    producer_df = extract_json_field_value(producer_df, 'offset', 'resp', '"base_offset":')
    consumer_df = extract_json_field_value(consumer_df, 'offset', 'req_body', '"fetch_offset":')

    # Keep only the commands with offsets.
    # Fetch requests that are a repeat of the previous one will not have offsets,
    # but we don't care about those.
    producer_df = producer_df[producer_df.offset != ""]
    consumer_df = consumer_df[consumer_df.offset != ""]

    # Self-join to match consumer requests with producer requests.
    df = consumer_df.merge(producer_df, how='inner', left_on='offset', right_on='offset', suffixes=['', '_producer'])
    df = df['time_', 'offset', 'time__producer']

    # Compute producer consumer latency.
    # If the consumer's fetch happened before the produce, then set latency to 0,
    # since it means the consumer is ready waiting for the produce as soon as it arrives.
    df.time_gap = (df.time_ - df.time__producer) / 1000.0 / 1000.0 / 1000.0
    df.time_gap = px.select(df.time_gap < 0.0, 0.0, df.time_gap)

    return df


# This needs to be re-written once PxL has better JSON querying support.
def extract_json_field_value(df, dest_col, src_col, field_name):
    len = px.length(field_name)
    df[dest_col] = px.substring(df[src_col], px.find(df[src_col], field_name), 100)
    df[dest_col] = px.substring(df[dest_col], len, px.find(df[dest_col], ',') - len)
    return df


def add_source_dest_columns(df):
    ''' Add source and destination columns for the Kafka request.

    Kafka requests are traced server-side (trace_role==2), unless the server is
    outside of the cluster in which case the request is traced client-side (trace_role==1).

    When trace_role==2, the Kafka request source is the remote_addr column
    and destination is the pod column. When trace_role==1, the Kafka request
    source is the pod column and the destination is the remote_addr column.

    Input DataFrame must contain trace_role, upid, remote_addr columns.
    '''
    df.pod = df.ctx['pod']
    df.namespace = df.ctx['namespace']

    # If remote_addr is a pod, get its name. If not, use IP address.
    df.ra_pod = px.pod_id_to_pod_name(px.ip_to_pod_id(df.remote_addr))
    df.is_ra_pod = df.ra_pod != ''
    df.ra_name = px.select(df.is_ra_pod, df.ra_pod, df.remote_addr)

    df.is_server_tracing = df.trace_role == 2
    df.is_source_pod_type = px.select(df.is_server_tracing, df.is_ra_pod, True)
    df.is_dest_pod_type = px.select(df.is_server_tracing, True, df.is_ra_pod)

    # Set source and destination based on trace_role.
    df.source = px.select(df.is_server_tracing, df.ra_name, df.pod)
    df.destination = px.select(df.is_server_tracing, df.pod, df.ra_name)

    # Filter out messages with empty source / destination.
    df = df[df.source != '' and df.source != '-']
    df = df[df.destination != '' and df.destination != '-']

    df = df.drop(['ra_pod', 'is_ra_pod', 'ra_name', 'is_server_tracing'])

    return df
