package controllers

import (
	"sync"
	"time"

	"github.com/cheekybits/genny/generic"
	log "github.com/sirupsen/logrus"
	v1 "k8s.io/api/core/v1"
	internalversion "k8s.io/apimachinery/pkg/apis/meta/internalversion"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/fields"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/watch"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/tools/cache"
	watchClient "k8s.io/client-go/tools/watch"

	// Blank import necessary for kubeConfig to work.
	_ "k8s.io/client-go/plugin/pkg/client/auth/gcp"

	protoutils "pixielabs.ai/pixielabs/src/shared/k8s"
	storepb "pixielabs.ai/pixielabs/src/vizier/services/metadata/storepb"
)

type ReplacedResource generic.Type

func runtimeObjToReplacedResourceList(o runtime.Object) *v1.ReplacedResourceList {
	l, ok := o.(*v1.ReplacedResourceList)
	if ok {
		return l
	}

	internalList, ok := o.(*internalversion.List)
	if !ok {
		log.WithField("list", o).Fatal("Received unexpected object type from lister")
	}

	typedList := v1.ReplacedResourceList{}
	for _, i := range internalList.Items {
		item, ok := i.(*v1.ReplacedResource)
		if !ok {
			log.WithField("expectedReplacedResource", i).Fatal("Received unexpected object type from lister")
		}
		typedList.Items = append(typedList.Items, *item)
	}

	return &typedList
}

// ReplacedResourceWatcher is a resource watcher for ReplacedResources.
type ReplacedResourceWatcher struct {
	resourceStr string
	lastRV      string
	updateCh    chan *K8sResourceMessage
	clientset   *kubernetes.Clientset
}

// NewReplacedResourceWatcher creates a resource watcher for ReplacedResources.
func NewReplacedResourceWatcher(resource string, updateCh chan *K8sResourceMessage, clientset *kubernetes.Clientset) *ReplacedResourceWatcher {
	return &ReplacedResourceWatcher{resourceStr: resource, updateCh: updateCh, clientset: clientset}
}

// Sync syncs the watcher state with the stored updates for ReplacedResources.
func (mc *ReplacedResourceWatcher) Sync(storedUpdates []*storepb.K8SResource) error {
	resources, err := listObject(mc.resourceStr, mc.clientset)
	if err != nil {
		return err
	}
	mc.syncReplacedResourceImpl(storedUpdates, runtimeObjToReplacedResourceList(resources))
	return nil
}

func (mc *ReplacedResourceWatcher) syncReplacedResourceImpl(storedUpdates []*storepb.K8SResource, currentState *v1.ReplacedResourceList) {
	activeResources := make(map[string]bool)

	// Send update for currently active resources. This will build our HostIP/PodIP maps.
	for _, o := range currentState.Items {
		pb, err := protoutils.ReplacedResourceToProto(&o)
		if err != nil {
			continue
		}
		activeResources[string(o.ObjectMeta.UID)] = true

		r := &storepb.K8SResource{
			Resource: &storepb.K8SResource_ReplacedResource{
				ReplacedResource: pb,
			},
		}
		msg := &K8sResourceMessage{
			Object:     r,
			ObjectType: mc.resourceStr,
			EventType:  watch.Modified,
		}
		mc.updateCh <- msg
	}
	mc.lastRV = currentState.ResourceVersion

	// Make a map of resources that we have stored.
	storedResources := make(map[string]*storepb.K8SResource)
	for i := range storedUpdates {
		update := storedUpdates[i].GetReplacedResource()
		if update == nil {
			continue
		}

		if update.Metadata.DeletionTimestampNS != 0 {
			// This resource is already terminated, so we don't
			// need to send another termination event.
			delete(storedResources, update.Metadata.UID)
			continue
		}

		storedResources[update.Metadata.UID] = storedUpdates[i]
	}

	// For each resource in our store, determine if it is still running. If not, send a termination update.
	for uid, r := range storedResources {
		if _, ok := activeResources[uid]; ok {
			continue
		}

		msg := &K8sResourceMessage{
			Object:     r,
			ObjectType: mc.resourceStr,
			EventType:  watch.Deleted,
		}
		mc.updateCh <- msg
	}
}

// StartWatcher starts a watcher for ReplacedResources.
func (mc *ReplacedResourceWatcher) StartWatcher(quitCh chan struct{}, wg *sync.WaitGroup) {
	defer wg.Done()
	for {
		watcher := cache.NewListWatchFromClient(mc.clientset.CoreV1().RESTClient(), mc.resourceStr, v1.NamespaceAll, fields.Everything())
		retryWatcher, err := watchClient.NewRetryWatcher(mc.lastRV, watcher)
		if err != nil {
			log.WithError(err).Fatal("Could not start watcher for k8s resource: " + mc.resourceStr)
		}

		resCh := retryWatcher.ResultChan()
		runWatcher := true
		for runWatcher {
			select {
			case <-quitCh:
				return
			case c := <-resCh:
				s, ok := c.Object.(*metav1.Status)
				if ok && s.Status == metav1.StatusFailure {
					if s.Reason == metav1.StatusReasonGone {
						log.WithField("resource", mc.resourceStr).Info("Requested resource version too old, no longer stored in K8S API")
						runWatcher = false
						break
					}
					// Ignore and let the retry watcher retry.
					log.WithField("resource", mc.resourceStr).WithField("object", c.Object).Info("Failed to read from k8s watcher")
					continue
				}

				// Update the lastRV, so that if the watcher restarts, it starts at the correct resource version.
				o, ok := c.Object.(*v1.ReplacedResource)
				if !ok {
					continue
				}

				mc.lastRV = o.ObjectMeta.ResourceVersion

				pb, err := protoutils.ReplacedResourceToProto(o)
				if err != nil {
					continue
				}
				r := &storepb.K8SResource{
					Resource: &storepb.K8SResource_ReplacedResource{
						ReplacedResource: pb,
					},
				}

				msg := &K8sResourceMessage{
					Object:     r,
					ObjectType: mc.resourceStr,
					EventType:  c.Type,
				}
				mc.updateCh <- msg
			}
		}

		log.WithField("resource", mc.resourceStr).Info("K8s watcher channel closed. Retrying")
		runWatcher = true

		// Wait 5 minutes before retrying, however if stop is called, just return.
		select {
		case <-quitCh:
			return
		case <-time.After(5 * time.Minute):
			continue
		}
	}
}
