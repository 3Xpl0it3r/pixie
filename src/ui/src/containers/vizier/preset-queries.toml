queries = [
["HTTP Requests Statistics per Service",
"""
t1 = dataframe(table='http_events', start_time='-30s')

t1['service'] = t1.attr['service']
t1['http_resp_latency_ms'] = t1['http_resp_latency_ns'] / 1.0E6
t1['failure'] = t1['http_resp_status'] >= 400
t1['range_group'] = t1['time_'] - pl.modulo(t1['time_'], 1000000000)

quantiles_agg = t1.groupby('service').agg(
  latency_quantiles=('http_resp_latency_ms', pl.quantiles),
  errors=('failure', pl.mean),
  throughput_total=('http_resp_status', pl.count),
)

quantiles_agg['latency_p50'] = pl.pluck(quantiles_agg['latency_quantiles'], 'p50')
quantiles_agg['latency_p90'] = pl.pluck(quantiles_agg['latency_quantiles'], 'p90')
quantiles_agg['latency_p99'] = pl.pluck(quantiles_agg['latency_quantiles'], 'p99')
quantiles_table = quantiles_agg[['service', 'latency_p50', 'latency_p90', 'latency_p99', 'errors', 'throughput_total']]

# The Range aggregate to calcualte the requests per second.
range_agg = t1.groupby(['service', 'range_group']).agg(
  requests_per_window=('http_resp_status', pl.count),
)

rps_table = range_agg.groupby('service').agg(rps=('requests_per_window',pl.mean))

joined_table = quantiles_table.merge(rps_table, 
                                     how='inner', 
                                     left_on=['service'], 
                                     right_on=['service'],
                                     suffixes=['', '_x'])

joined_table['latency(p50)'] = joined_table['latency_p50']
joined_table['latency(p90)'] = joined_table['latency_p90']
joined_table['latency(p99)'] = joined_table['latency_p99']
joined_table['throughput (rps)'] = joined_table['rps']
joined_table['throughput total'] = joined_table['throughput_total']

joined_table = joined_table[[
  'service',
  'latency(p50)',
  'latency(p90)',
  'latency(p99)',
  'errors',
  'throughput (rps)',
  'throughput total']]
joined_table[joined_table['service'] != ''].result(name='out')
"""
],
["Pod Resource Usage In Time Interval",
"""
# 
# Returns the Resource Usage for each pod during an interval.
# 
t1 = dataframe(table='process_stats', start_time='-60s')

# Convert to better units.
t1['cpu_utime_s'] = t1['cpu_utime_ns'] / 1.0E9
t1['cpu_ktime_s'] = t1['cpu_ktime_ns'] / 1.0E9
t1['vsize_mb'] = t1['vsize_bytes'] / 1024.0 / 1024.0
t1['rss_bytes_mb'] = t1['rss_bytes'] / 1024.0 / 1024.0
t1['read_bytes_mb'] = t1['read_bytes'] / 1024.0 / 1024.0
t1['write_bytes_mb'] = t1['write_bytes'] / 1024.0 / 1024.0
t1['rchar_bytes_mb'] = t1['rchar_bytes'] / 1024.0 / 1024.0
t1['wchar_bytes_mb'] = t1['wchar_bytes'] / 1024.0 / 1024.0

upid_aggop = t1.groupby('upid').agg(
  vsize_mb=('vsize_mb', pl.mean),
  rss_bytes_mb=('rss_bytes_mb', pl.mean),
  # The following columns are all counters, so we diff the value at the beginning 
  # of the period with the value at the end of the period.
  cpu_utime_s_end=('cpu_utime_s', pl.max),
  cpu_ktime_s_end=('cpu_ktime_s', pl.max),
  cpu_utime_s_start=('cpu_utime_s', pl.min),
  cpu_ktime_s_start=('cpu_ktime_s', pl.min),
  read_bytes_mb_end=('read_bytes_mb', pl.max),
  read_bytes_mb_start=('read_bytes_mb', pl.min),
  write_bytes_mb_end=('write_bytes_mb', pl.max),
  write_bytes_mb_start=('write_bytes_mb', pl.min),
  rchar_bytes_mb_end=('rchar_bytes_mb', pl.max),
  rchar_bytes_mb_start=('rchar_bytes_mb', pl.min),
  wchar_bytes_mb_end=('wchar_bytes_mb', pl.max),
  wchar_bytes_mb_start=('wchar_bytes_mb', pl.min),
)

# The cpu time diff.
upid_aggop['cpu_utime_s'] = upid_aggop['cpu_utime_s_end'] - upid_aggop['cpu_utime_s_start']
upid_aggop['cpu_ktime_s'] = upid_aggop['cpu_ktime_s_end'] - upid_aggop['cpu_ktime_s_start']
upid_aggop['read_bytes_mb'] = upid_aggop['read_bytes_mb_end'] - upid_aggop['read_bytes_mb_start']
upid_aggop['write_bytes_mb'] = upid_aggop['write_bytes_mb_end'] - upid_aggop['write_bytes_mb_start']
upid_aggop['rchar_bytes_mb'] = upid_aggop['rchar_bytes_mb_end'] - upid_aggop['rchar_bytes_mb_start']
upid_aggop['wchar_bytes_mb'] = upid_aggop['wchar_bytes_mb_end'] - upid_aggop['wchar_bytes_mb_start']
upid_aggop['pod'] = upid_aggop.attr['pod']

# For this aggregate, we sum up the values as we've already calculated the average/usage 
# for the upids already.
pod_aggop = upid_aggop.groupby('pod').agg(
  cpu_utime_s=('cpu_utime_s', pl.sum),
  cpu_ktime_s=('cpu_ktime_s', pl.sum),
  vsize_mb=('vsize_mb', pl.sum),
  rss_bytes_mb=('rss_bytes_mb', pl.sum),
  read_bytes_mb=('read_bytes_mb', pl.sum),
  write_bytes_mb=('write_bytes_mb', pl.sum),
  rchar_bytes_mb=('rchar_bytes_mb', pl.sum),
  wchar_bytes_mb=('wchar_bytes_mb', pl.sum),
)

# Format everything nicely.
pod_aggop['pod_name'] = pod_aggop['pod']
pod_aggop['status'] = pl.pod_name_to_status(pod_aggop['pod_name'])
pod_aggop['Created on'] = pl.pod_name_to_start_time(pod_aggop['pod_name'])
pod_aggop['CPU User time (s)'] = pod_aggop['cpu_utime_s']
pod_aggop['CPU System time (s)'] = pod_aggop['cpu_ktime_s']
pod_aggop['Virtual Memory (mb)'] = pod_aggop['vsize_mb']
pod_aggop['Average Memory (mb)'] = pod_aggop['rss_bytes_mb']
pod_aggop['Read to IO (mb)'] = pod_aggop['read_bytes_mb']
pod_aggop['Write to IO (mb)'] = pod_aggop['write_bytes_mb']
pod_aggop['Characters Read (mb)'] = pod_aggop['rchar_bytes_mb']
pod_aggop['Characters written (mb)'] = pod_aggop['wchar_bytes_mb']

keep_columns = pod_aggop[['pod_name', 'status', 'Created on', 'CPU User time (s)', 'CPU System time (s)',
                          'Virtual Memory (mb)', 'Average Memory (mb)', 'Read to IO (mb)', 'Write to IO (mb)',
                          'Characters Read (mb)', 'Characters written (mb)']]
keep_columns.result(name='out')
"""],
["Total Resource Usage During Pod Lifetime",
"""
# 
# Returns the Total Resource Usage for each pod.
# 
t1 = dataframe(table='process_stats', start_time='-60s')

# Convert to better units.
t1['cpu_utime_s'] = t1['cpu_utime_ns'] / 1.0E9
t1['cpu_ktime_s'] = t1['cpu_ktime_ns'] / 1.0E9
t1['vsize_mb'] = t1['vsize_bytes'] / 1024.0 / 1024.0
t1['rss_bytes_mb'] = t1['rss_bytes'] / 1024.0 / 1024.0
t1['read_bytes_mb'] = t1['read_bytes'] / 1024.0 / 1024.0
t1['write_bytes_mb'] = t1['write_bytes'] / 1024.0 / 1024.0
t1['rchar_bytes_mb'] = t1['rchar_bytes'] / 1024.0 / 1024.0
t1['wchar_bytes_mb'] = t1['wchar_bytes'] / 1024.0 / 1024.0

upid_aggop = t1.groupby('upid').agg(
  vsize_mb=('vsize_mb', pl.mean),
  rss_bytes_mb=('rss_bytes_mb', pl.mean),
  # The following columns are all counters, so we take the maximum value.
  cpu_utime_s=('cpu_utime_s', pl.max),
  cpu_ktime_s=('cpu_ktime_s', pl.max),
  read_bytes_mb=('read_bytes_mb', pl.max),
  write_bytes_mb=('write_bytes_mb', pl.max),
  rchar_bytes_mb=('rchar_bytes_mb', pl.max),
  wchar_bytes_mb=('wchar_bytes_mb', pl.max),
)
# For this aggregate, we sum up the values as we've already calculated the average/usage 
# for the upids already.
upid_aggop['pod'] = upid_aggop.attr['pod']
pod_aggop = upid_aggop.groupby('pod').agg(
  cpu_utime_s=('cpu_utime_s', pl.sum),
  cpu_ktime_s=('cpu_ktime_s', pl.sum),
  vsize_mb=('vsize_mb', pl.sum),
  rss_bytes_mb=('rss_bytes_mb', pl.sum),
  read_bytes_mb=('read_bytes_mb', pl.sum),
  write_bytes_mb=('write_bytes_mb', pl.sum),
  rchar_bytes_mb=('rchar_bytes_mb', pl.sum),
  wchar_bytes_mb=('wchar_bytes_mb', pl.sum),
)

# Format everything nicely.
pod_aggop['pod_name'] = pod_aggop['pod']
pod_aggop['status'] = pl.pod_name_to_status(pod_aggop['pod_name'])
pod_aggop['Created on'] = pl.pod_name_to_start_time(pod_aggop['pod_name'])
pod_aggop['CPU User time (s)'] = pod_aggop['cpu_utime_s']
pod_aggop['CPU System time (s)'] = pod_aggop['cpu_ktime_s']
pod_aggop['Virtual Memory (mb)'] = pod_aggop['vsize_mb']
pod_aggop['Average Memory (mb)'] = pod_aggop['rss_bytes_mb']
pod_aggop['Read to IO (mb)'] = pod_aggop['read_bytes_mb']
pod_aggop['Write to IO (mb)'] = pod_aggop['write_bytes_mb']
pod_aggop['Characters Read (mb)'] = pod_aggop['rchar_bytes_mb']
pod_aggop['Characters written (mb)'] = pod_aggop['wchar_bytes_mb']

keep_columns = pod_aggop[['pod_name', 'status', 'Created on', 'CPU User time (s)', 'CPU System time (s)',
                          'Virtual Memory (mb)', 'Average Memory (mb)', 'Read to IO (mb)', 'Write to IO (mb)',
                          'Characters Read (mb)', 'Characters written (mb)']]
keep_columns.result(name='out')
"""],
["Sample HTTP Data",
"""
t1 = dataframe(table='http_events', select=['time_', 'remote_addr', 'remote_port', 'http_resp_status', 'http_resp_message', 'http_resp_body', 'http_resp_latency_ns'], start_time='-30s')
t1['http_resp_latency_ms'] = t1['http_resp_latency_ns'] / 1.0E6
t2 = t1.drop(columns=['http_resp_latency_ns']).limit(rows=100).result(name='resp_table')
"""],
["Sample MySQL Data",
"""
t1 = dataframe(table='mysql_events', select=['time_', 'remote_addr', 'remote_port', 'req_cmd', 'req_body', 'resp_status', 'resp_body', 'latency_ns'], start_time='-30s')
t1['latency_ms'] = t1['latency_ns'] / 1.0E6
t2 = t1.drop(columns=['latency_ns']).limit(rows=100).result(name='resp_table')
"""],
["Network Stats",
"""
t1 = dataframe(table='network_stats', select=['time_', 'pod_id', 'rx_bytes', 'rx_packets', 'rx_errors',
                                         'rx_drops', 'tx_bytes', 'tx_packets', 'tx_errors', 'tx_drops'], start_time='-30s')
t2 = t1.limit(rows=100).result(name='stats')
"""]
]
