queries = [
["Service HTTP Requests",
"""
t1 = dataframe(table='http_events').range(start='-30s')

mapop = t1.map(fn=lambda r: {
  'time_': r.time_,
  'upid': r.upid,
  'service': r.attr.service,
  'remote_addr': r.remote_addr,
  'remote_port': r.remote_port,
  'http_resp_status': r.http_resp_status,
  'http_resp_message': r.http_resp_message,
  'http_resp_latency_ms': r.http_resp_latency_ns / 1.0E6,
  'failure': r.http_resp_status >= 400,
  
  'range_group': pl.subtract(r.time_, pl.modulo(r.time_, 1000000000)),
})
quantiles_agg = mapop.agg(by=lambda r: [r.attr.service], fn=lambda r: {
  'latency_quantiles': pl.quantiles(r.http_resp_latency_ms),
  'errors': pl.mean(r.failure),
  'throughput_total': pl.count(r.http_resp_status),
})
quantiles_table = quantiles_agg.map(fn=lambda r:{
  'service': r.attr.service,
  'latency_p50': pl.pluck(r.latency_quantiles, 'p50'),
  'latency_p90': pl.pluck(r.latency_quantiles, 'p90'),
  'latency_p99': pl.pluck(r.latency_quantiles, 'p99'),
  'errors': r.errors,
  'throughput_total': r.throughput_total,
})

# The Range aggregate to calcualte the requests per second.
range_agg = mapop.agg(by=lambda r: [r.attr.service, r.range_group], fn=lambda r: {
  'requests_per_window': pl.count(r.http_resp_status)
})

rps_table = range_agg.agg(by=lambda r: r.attr.service, fn= lambda r: {'rps': pl.mean(r.requests_per_window)})
joined_table = quantiles_table.merge(rps_table,  type='inner',
                                    cond=lambda r1, r2: r1.service == r2._attr_service_name,
                                    cols=lambda r1, r2: {
                                      "service" : r1.service,
                                      'latency(p50)': r1.latency_p50,
                                      'latency(p90)': r1.latency_p90,
                                      'latency(p99)': r1.latency_p99,
                                      'errors': r1.errors,
                                      'throughput (rps)': r2.rps,
                                      'throughput total': r1.throughput_total,
                                    })
joined_table.filter(fn=lambda r: r.service != "").result(name="out")
"""
],
["Pod Resource Usage In Time Interval",
"""
# 
# Returns the Resource Usage for each pod during an interval.
# 
t1 = dataframe(table='process_stats').range(start='-60s')
# Convert to better units.
mapop = t1.map(fn=lambda r:{
  'time_': r.time_,
  'upid': r.upid,
  'cpu_utime_s': r.cpu_utime_ns / 1.0E9,
  'cpu_ktime_s': r.cpu_ktime_ns / 1.0E9,
  'vsize_mb': r.vsize_bytes / 1024.0 / 1024.0,
  'rss_bytes_mb': r.rss_bytes / 1024.0 / 1024.0,
  'read_bytes_mb': r.read_bytes / 1024.0 / 1024.0,
  'write_bytes_mb': r.write_bytes / 1024.0 / 1024.0,
  'rchar_bytes_mb': r.rchar_bytes / 1024.0 / 1024.0, 
  'wchar_bytes_mb': r.wchar_bytes / 1024.0 / 1024.0,
})

upid_aggop = mapop.agg(by=lambda r: r.upid, fn=lambda r:{
  'vsize_mb': pl.mean(r.vsize_mb),
  'rss_bytes_mb': pl.mean(r.rss_bytes_mb),
  # The following columns are all counters, so we diff the value at the beginning 
  # of the period with the value at the end of the period.
  'cpu_utime_s_end': pl.max(r.cpu_utime_s),
  'cpu_ktime_s_end': pl.max(r.cpu_ktime_s),
  'cpu_utime_s_start': pl.min(r.cpu_utime_s),
  'cpu_ktime_s_start': pl.min(r.cpu_ktime_s),
  'read_bytes_mb_end': pl.max(r.read_bytes_mb),
  'read_bytes_mb_start': pl.min(r.read_bytes_mb),
  'write_bytes_mb_end': pl.max(r.write_bytes_mb),
  'write_bytes_mb_start': pl.min(r.write_bytes_mb),
  "rchar_bytes_mb_end": pl.max(r.rchar_bytes_mb),
  "rchar_bytes_mb_start": pl.min(r.rchar_bytes_mb),
  "wchar_bytes_mb_end": pl.max(r.wchar_bytes_mb),
  "wchar_bytes_mb_start": pl.min(r.wchar_bytes_mb)
})
upid_aggop= upid_aggop.map(fn=lambda r:{
  'upid': r.upid,
  # The cpu time diff.
  'cpu_utime_s': r.cpu_utime_s_end - r.cpu_utime_s_start,
  'cpu_ktime_s': r.cpu_ktime_s_end - r.cpu_ktime_s_start,
  'vsize_mb': r.vsize_mb,
  'rss_bytes_mb': r.rss_bytes_mb,
  'read_bytes_mb': r.read_bytes_mb_end - r.read_bytes_mb_start,
  'write_bytes_mb': r.write_bytes_mb_end - r.write_bytes_mb_start,
  "rchar_bytes_mb": r.rchar_bytes_mb_end - r.rchar_bytes_mb_start,
  "wchar_bytes_mb": r.wchar_bytes_mb_end - r.wchar_bytes_mb_start,
})
# For this aggregate, we sum up the values as we've already calculated the average/usage 
# for the upids already.
pod_aggop = upid_aggop.agg(by=lambda r: r.attr.pod, fn=lambda r:{
  'cpu_utime_s': pl.sum(r.cpu_utime_s),
  'cpu_ktime_s': pl.sum(r.cpu_ktime_s),
  'vsize_mb': pl.sum(r.vsize_mb),
  'rss_bytes_mb': pl.sum(r.rss_bytes_mb),
  'read_bytes_mb': pl.sum(r.read_bytes_mb),
  'write_bytes_mb': pl.sum(r.write_bytes_mb),
  "rchar_bytes_mb": pl.sum(r.rchar_bytes_mb),
  "wchar_bytes_mb": pl.sum(r.wchar_bytes_mb)
})
# Format everything nicely.
rename = pod_aggop.map(fn=lambda r:{
  "pod_name": r.attr.pod,
  "status": pl.pod_name_to_status(r._attr_pod_name),
  "Created on": pl.pod_name_to_start_time(r._attr_pod_name),
  "CPU User time (s)": r.cpu_utime_s,
  "CPU System time (s)": r.cpu_ktime_s,
  "Virtual Memory (mb)": r.vsize_mb,
  "Average Memory (mb)": r.rss_bytes_mb,
  "Read to IO (mb)": r.read_bytes_mb,
  "Write to IO (mb)": r.write_bytes_mb,
  "Characters Read (mb)": r.rchar_bytes_mb,
  "Characters written (mb)": r.wchar_bytes_mb,
})
rename.result(name='out')
"""],
["Total Resource Usage During Pod Lifetime",
"""
# 
# Returns the Total Resource Usage for each pod.
# 
t1 = dataframe(table='process_stats').range(start='-60s')
# Convert to better units.
mapop = t1.map(fn=lambda r:{
  'time_': r.time_,
  'upid': r.upid,
  'cpu_utime_s': r.cpu_utime_ns / 1.0E9,
  'cpu_ktime_s': r.cpu_ktime_ns / 1.0E9,
  'vsize_mb': r.vsize_bytes / 1024.0 / 1024.0,
  'rss_bytes_mb': r.rss_bytes / 1024.0 / 1024.0,
  'read_bytes_mb': r.read_bytes / 1024.0 / 1024.0,
  'write_bytes_mb': r.write_bytes / 1024.0 / 1024.0,
  'rchar_bytes_mb': r.rchar_bytes / 1024.0 / 1024.0, 
  'wchar_bytes_mb': r.wchar_bytes / 1024.0 / 1024.0,
})

upid_aggop = mapop.agg(by=lambda r: r.upid, fn=lambda r:{
  'vsize_mb': pl.mean(r.vsize_mb),
  'rss_bytes_mb': pl.mean(r.rss_bytes_mb),
  # The following columns are all counters, so we take the maximum value.
  'cpu_utime_s': pl.max(r.cpu_utime_s),
  'cpu_ktime_s': pl.max(r.cpu_ktime_s),
  'read_bytes_mb': pl.max(r.read_bytes_mb),
  'write_bytes_mb': pl.max(r.write_bytes_mb),
  "rchar_bytes_mb": pl.max(r.rchar_bytes_mb),
  "wchar_bytes_mb": pl.max(r.wchar_bytes_mb),
})
# For this aggregate, we sum up the values as we've already calculated the average/usage 
# for the upids already.
pod_aggop = upid_aggop.agg(by=lambda r: r.attr.pod, fn=lambda r:{
  'cpu_utime_s': pl.sum(r.cpu_utime_s),
  'cpu_ktime_s': pl.sum(r.cpu_ktime_s),
  'vsize_mb': pl.sum(r.vsize_mb),
  'rss_bytes_mb': pl.sum(r.rss_bytes_mb),
  'read_bytes_mb': pl.sum(r.read_bytes_mb),
  'write_bytes_mb': pl.sum(r.write_bytes_mb),
  "rchar_bytes_mb": pl.sum(r.rchar_bytes_mb),
  "wchar_bytes_mb": pl.sum(r.wchar_bytes_mb)
})
# Format everything nicely.
rename = pod_aggop.map(fn=lambda r:{
  "pod_name": r.attr.pod,
  "status": pl.pod_name_to_status(r._attr_pod_name),
  "Created on": pl.pod_name_to_start_time(r._attr_pod_name),
  "CPU User time (s)": r.cpu_utime_s,
  "CPU System time (s)": r.cpu_ktime_s,
  "Virtual Memory (mb)": r.vsize_mb,
  "Average Memory (mb)": r.rss_bytes_mb,
  "Read to IO (mb)": r.read_bytes_mb,
  "Write to IO (mb)": r.write_bytes_mb,
  "Characters Read (mb)": r.rchar_bytes_mb,
  "Characters written (mb)": r.wchar_bytes_mb,
})
rename.result(name='out')
"""],
["Sample HTTP Data",
"""
t1 = dataframe(table='http_events', select=['time_', 'remote_addr', 'remote_port', 'http_resp_status', 'http_resp_message', 'http_resp_body', 'http_resp_latency_ns']).range(start='-30s')
t2 = t1.map(fn=lambda r: {'time_': r.time_,
                       'remote_addr': r.remote_addr,
                       'remote_port': r.remote_port,
                       'http_resp_status': r.http_resp_status,
                       'http_resp_message': r.http_resp_message,
                       'http_resp_body': r.http_resp_body,
                       'http_resp_latency_ms': r.http_resp_latency_ns / 1.0E6})
t3 = t2.limit(rows=100).result(name='resp_table')
"""],
["Sample MySQL Data",
"""
t1 = dataframe(table='mysql_events', select=['time_', 'remote_addr', 'remote_port', 'req_cmd', 'req_body', 'resp_status', 'resp_body', 'latency_ns']).range(start='-30s')
t2 = t1.map(fn=lambda r: {'time_': r.time_,
                       'remote_addr': r.remote_addr,
                       'remote_port': r.remote_port,
                       'req_cmd': r.req_cmd,
                       'req_body': r.req_body,
                       'resp_status': r.resp_status,
                       'resp_body' : r.resp_body,
                       'latency_ns': r.latency_ns / 1.0E6})
t3 = t2.limit(rows=100).result(name='resp_table')
"""],
["Network Stats",
"""
t1 = dataframe(table='network_stats', select=['time_', 'pod_id', 'rx_bytes', 'rx_packets', 'rx_errors',
                                         'rx_drops', 'tx_bytes', 'tx_packets', 'tx_errors', 'tx_drops']).range(start='-30s')
t2 = t1.limit(rows=100).result(name='stats')
"""]
]
