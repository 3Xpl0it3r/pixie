queries = [
["HTTP Requests Statistics per Service",
"""
t1 = pl.DataFrame(table='http_events', start_time='-30s')

t1.service = t1.attr['service']
t1.http_resp_latency_ms = t1.http_resp_latency_ns / 1.0E6
t1.failure = t1.http_resp_status >= 400
t1.range_group = t1.time_ - pl.modulo(t1.time_, 1000000000)

quantiles_agg = t1.groupby('service').agg(
  latency_quantiles=('http_resp_latency_ms', pl.quantiles),
  errors=('failure', pl.mean),
  throughput_total=('http_resp_status', pl.count),
)

quantiles_agg.latency_p50 = pl.pluck(quantiles_agg.latency_quantiles, 'p50')
quantiles_agg.latency_p90 = pl.pluck(quantiles_agg.latency_quantiles, 'p90')
quantiles_agg.latency_p99 = pl.pluck(quantiles_agg.latency_quantiles, 'p99')
quantiles_table = quantiles_agg[['service', 'latency_p50', 'latency_p90', 'latency_p99', 'errors', 'throughput_total']]

# The Range aggregate to calcualte the requests per second.
range_agg = t1.groupby(['service', 'range_group']).agg(
  requests_per_window=('http_resp_status', pl.count),
)

rps_table = range_agg.groupby('service').agg(rps=('requests_per_window',pl.mean))

joined_table = quantiles_table.merge(rps_table,
                                     how='inner',
                                     left_on=['service'],
                                     right_on=['service'],
                                     suffixes=['', '_x'])

joined_table['latency(p50)'] = joined_table.latency_p50
joined_table['latency(p90)'] = joined_table.latency_p90
joined_table['latency(p99)'] = joined_table.latency_p99
joined_table['throughput (rps)'] = joined_table.rps
joined_table['throughput total'] = joined_table.throughput_total

joined_table = joined_table[[
  'service',
  'latency(p50)',
  'latency(p90)',
  'latency(p99)',
  'errors',
  'throughput (rps)',
  'throughput total']]
joined_table = joined_table[joined_table.service != '']
pl.display(joined_table)
"""
],
["Pod Resource Usage In Time Interval",
"""
#
# Returns the Resource Usage for each pod during an interval.
#
t1 = pl.DataFrame(table='process_stats', start_time='-60s')

bytes_per_mb = 1024.0 * 1024.0

# Convert to better units.
t1.cpu_utime_s = t1.cpu_utime_ns / 1.0E9
t1.cpu_ktime_s = t1.cpu_ktime_ns / 1.0E9
t1.vsize_mb = t1.vsize_bytes / bytes_per_mb
t1.rss_bytes_mb = t1.rss_bytes / bytes_per_mb
t1.read_bytes_mb = t1.read_bytes / bytes_per_mb
t1.write_bytes_mb = t1.write_bytes / bytes_per_mb
t1.rchar_bytes_mb = t1.rchar_bytes / bytes_per_mb
t1.wchar_bytes_mb = t1.wchar_bytes / bytes_per_mb
t1.pod = t1.attr['pod']

upid_aggop = t1.groupby(['upid', 'pod']).agg(
  vsize_mb=('vsize_mb', pl.mean),
  rss_bytes_mb=('rss_bytes_mb', pl.mean),
  # The following columns are all counters, so we diff the value at the beginning
  # of the period with the value at the end of the period.
  cpu_utime_s_end=('cpu_utime_s', pl.max),
  cpu_ktime_s_end=('cpu_ktime_s', pl.max),
  cpu_utime_s_start=('cpu_utime_s', pl.min),
  cpu_ktime_s_start=('cpu_ktime_s', pl.min),
  read_bytes_mb_end=('read_bytes_mb', pl.max),
  read_bytes_mb_start=('read_bytes_mb', pl.min),
  write_bytes_mb_end=('write_bytes_mb', pl.max),
  write_bytes_mb_start=('write_bytes_mb', pl.min),
  rchar_bytes_mb_end=('rchar_bytes_mb', pl.max),
  rchar_bytes_mb_start=('rchar_bytes_mb', pl.min),
  wchar_bytes_mb_end=('wchar_bytes_mb', pl.max),
  wchar_bytes_mb_start=('wchar_bytes_mb', pl.min),
)

# The cpu time diff.
upid_aggop.cpu_utime_s = upid_aggop.cpu_utime_s_end - upid_aggop.cpu_utime_s_start
upid_aggop.cpu_ktime_s = upid_aggop.cpu_ktime_s_end - upid_aggop.cpu_ktime_s_start
upid_aggop.read_bytes_mb = upid_aggop.read_bytes_mb_end - upid_aggop.read_bytes_mb_start
upid_aggop.write_bytes_mb = upid_aggop.write_bytes_mb_end - upid_aggop.write_bytes_mb_start
upid_aggop.rchar_bytes_mb = upid_aggop.rchar_bytes_mb_end - upid_aggop.rchar_bytes_mb_start
upid_aggop.wchar_bytes_mb = upid_aggop.wchar_bytes_mb_end - upid_aggop.wchar_bytes_mb_start

# For this aggregate, we sum up the values as we've already calculated the average/usage
# for the upids already.
pod_aggop = upid_aggop.groupby('pod').agg(
  cpu_utime_s=('cpu_utime_s', pl.sum),
  cpu_ktime_s=('cpu_ktime_s', pl.sum),
  vsize_mb=('vsize_mb', pl.sum),
  rss_bytes_mb=('rss_bytes_mb', pl.sum),
  read_bytes_mb=('read_bytes_mb', pl.sum),
  write_bytes_mb=('write_bytes_mb', pl.sum),
  rchar_bytes_mb=('rchar_bytes_mb', pl.sum),
  wchar_bytes_mb=('wchar_bytes_mb', pl.sum),
)

# Format everything nicely.
pod_aggop.pod_name = pod_aggop.pod
pod_aggop.status = pl.pod_name_to_status(pod_aggop.pod_name)
pod_aggop['Created on'] = pl.pod_name_to_start_time(pod_aggop.pod_name)
pod_aggop['CPU User time (s)'] = pod_aggop.cpu_utime_s
pod_aggop['CPU System time (s)'] = pod_aggop.cpu_ktime_s
pod_aggop['Virtual Memory (mb)'] = pod_aggop.vsize_mb
pod_aggop['Average Memory (mb)'] = pod_aggop.rss_bytes_mb
pod_aggop['Read to IO (mb)'] = pod_aggop.read_bytes_mb
pod_aggop['Write to IO (mb)'] = pod_aggop.write_bytes_mb
pod_aggop['Characters Read (mb)'] = pod_aggop.rchar_bytes_mb
pod_aggop['Characters written (mb)'] = pod_aggop.wchar_bytes_mb

keep_columns = pod_aggop[['pod_name', 'status', 'Created on', 'CPU User time (s)', 'CPU System time (s)',
                          'Virtual Memory (mb)', 'Average Memory (mb)', 'Read to IO (mb)', 'Write to IO (mb)',
                          'Characters Read (mb)', 'Characters written (mb)']]
pl.display(keep_columns)
"""],
["Total Resource Usage During Pod Lifetime",
"""
#
# Returns the Total Resource Usage for each pod.
#
t1 = pl.DataFrame(table='process_stats', start_time='-60s')

bytes_per_mb = 1024.0 * 1024.0

# Convert to better units.
t1.cpu_utime_s = t1.cpu_utime_ns / 1.0E9
t1.cpu_ktime_s = t1.cpu_ktime_ns / 1.0E9
t1.vsize_mb = t1.vsize_bytes / bytes_per_mb
t1.rss_bytes_mb = t1.rss_bytes / bytes_per_mb
t1.read_bytes_mb = t1.read_bytes / bytes_per_mb
t1.write_bytes_mb = t1.write_bytes / bytes_per_mb
t1.rchar_bytes_mb = t1.rchar_bytes / bytes_per_mb
t1.wchar_bytes_mb = t1.wchar_bytes / bytes_per_mb
t1.pod  = t1.attr['pod']

upid_aggop = t1.groupby(['upid', 'pod']).agg(
  vsize_mb=('vsize_mb', pl.mean),
  rss_bytes_mb=('rss_bytes_mb', pl.mean),
  # The following columns are all counters, so we take the maximum value.
  cpu_utime_s=('cpu_utime_s', pl.max),
  cpu_ktime_s=('cpu_ktime_s', pl.max),
  read_bytes_mb=('read_bytes_mb', pl.max),
  write_bytes_mb=('write_bytes_mb', pl.max),
  rchar_bytes_mb=('rchar_bytes_mb', pl.max),
  wchar_bytes_mb=('wchar_bytes_mb', pl.max),
)
# For this aggregate, we sum up the values as we've already calculated the average/usage
# for the upids already.
pod_aggop = upid_aggop.groupby('pod').agg(
  cpu_utime_s=('cpu_utime_s', pl.sum),
  cpu_ktime_s=('cpu_ktime_s', pl.sum),
  vsize_mb=('vsize_mb', pl.sum),
  rss_bytes_mb=('rss_bytes_mb', pl.sum),
  read_bytes_mb=('read_bytes_mb', pl.sum),
  write_bytes_mb=('write_bytes_mb', pl.sum),
  rchar_bytes_mb=('rchar_bytes_mb', pl.sum),
  wchar_bytes_mb=('wchar_bytes_mb', pl.sum),
)

# Format everything nicely.
pod_aggop.pod_name = pod_aggop.pod
pod_aggop.status = pl.pod_name_to_status(pod_aggop.pod_name)
pod_aggop['Created on'] = pl.pod_name_to_start_time(pod_aggop.pod_name)
pod_aggop['CPU User time (s)'] = pod_aggop.cpu_utime_s
pod_aggop['CPU System time (s)'] = pod_aggop.cpu_ktime_s
pod_aggop['Virtual Memory (mb)'] = pod_aggop.vsize_mb
pod_aggop['Average Memory (mb)'] = pod_aggop.rss_bytes_mb
pod_aggop['Read to IO (mb)'] = pod_aggop.read_bytes_mb
pod_aggop['Write to IO (mb)'] = pod_aggop.write_bytes_mb
pod_aggop['Characters Read (mb)'] = pod_aggop.rchar_bytes_mb
pod_aggop['Characters written (mb)'] = pod_aggop.wchar_bytes_mb

keep_columns = pod_aggop[['pod_name', 'status', 'Created on', 'CPU User time (s)', 'CPU System time (s)',
                          'Virtual Memory (mb)', 'Average Memory (mb)', 'Read to IO (mb)', 'Write to IO (mb)',
                          'Characters Read (mb)', 'Characters written (mb)']]
pl.display(keep_columns)
"""],
["Sample HTTP Data",
"""
t1 = pl.DataFrame(table='http_events', select=['time_', 'remote_addr', 'remote_port', 'http_resp_status', 'http_resp_message', 'http_resp_body', 'http_resp_latency_ns'], start_time='-30s')
t1.http_resp_latency_ms = t1.http_resp_latency_ns / 1.0E6
t2 = t1.drop(columns=['http_resp_latency_ns']).head(n=100)
pl.display(t2)
"""],
["Sample MySQL Data",
"""
t1 = pl.DataFrame(table='mysql_events', select=['time_', 'remote_addr', 'remote_port', 'req_cmd', 'req_body', 'resp_status', 'resp_body', 'latency_ns'], start_time='-30s')
t1.latency_ms = t1.latency_ns / 1.0E6
t2 = t1.drop(columns=['latency_ns']).head(n=100)
pl.display(t2)
"""],
["Network Stats",
"""
t1 = pl.DataFrame(table='network_stats', select=['time_', 'pod_id', 'rx_bytes', 'rx_packets', 'rx_errors',
                                         'rx_drops', 'tx_bytes', 'tx_packets', 'tx_errors', 'tx_drops'], start_time='-30s')
t2 = t1.head(n=100)
pl.display(t2)
"""],
["Memory Usage of Pods",
"""
#
# Get the Virtual memory usage and average memory for all pods in the k8s cluster.
#
t1 = pl.DataFrame(table='process_stats', start_time='-30m')
bytes_per_mb = 1024.0 * 1024.0
t1.vsize_mb = t1.vsize_bytes / bytes_per_mb
t1.rss_bytes_mb = t1.rss_bytes / bytes_per_mb
t1.timestamp = pl.bin(t1.time_, pl.seconds(10))
t1.pod = t1.attr['pod']
# uncomment and replace service_name to filter on a specific service.
#t1 = t1[pl.contains(t1.service, 'service_name')]
upid_aggop = t1.groupby(['upid', 'pod', 'timestamp']).agg(
  vsize_mb=('vsize_mb', pl.mean),
  rss_bytes_mb=('rss_bytes_mb', pl.mean),
)
# For this aggregate, we sum up the values as we've already calculated the average/usage
# for the upids already, just need to do it for the entire pod.
pod_aggop = upid_aggop.groupby(['pod', 'timestamp']).agg(
  vsize_mb=('vsize_mb', pl.sum),
  rss_bytes_mb=('rss_bytes_mb', pl.sum),
)
# Format column names.
pod_aggop['Virtual Memory (mb)'] = pod_aggop.vsize_mb
pod_aggop['Average Memory (mb)'] = pod_aggop.rss_bytes_mb
keep_columns = pod_aggop[[
  'pod',
  'timestamp',
  'Virtual Memory (mb)',
  'Average Memory (mb)'
]]
pl.display(keep_columns)
"""],
["Memory Usage of Services",
"""
#
# Get the Virtual memory usage and average memory for all services in the k8s cluster.
#
t1 = pl.DataFrame(table='process_stats', start_time='-30m')
bytes_per_mb = 1024.0 * 1024.0
t1.vsize_mb = t1.vsize_bytes / bytes_per_mb
t1.rss_bytes_mb = t1.rss_bytes / bytes_per_mb
t1.timestamp = pl.bin(t1.time_, pl.seconds(10))
t1.service  = t1.attr['service']
# uncomment and replace service_name to filter on a specific service.
#t1 = t1[pl.contains(t1.service, 'service_name')]
upid_aggop = t1.groupby(['upid', 'service', 'timestamp']).agg(
  vsize_mb=('vsize_mb', pl.mean),
  rss_bytes_mb=('rss_bytes_mb', pl.mean),
)
# For this aggregate, we sum up the values as we've already calculated the average/usage
# for the upids already, just need to do it for the entire service.
aggop = upid_aggop.groupby(['service', 'timestamp']).agg(
  vsize_mb=('vsize_mb', pl.sum),
  rss_bytes_mb=('rss_bytes_mb', pl.sum),
)
# Format column names.
aggop['Virtual Memory (mb)'] = aggop.vsize_mb
aggop['Average Memory (mb)'] = aggop.rss_bytes_mb
keep_columns = aggop[[
  'service',
  'timestamp',
  'Virtual Memory (mb)',
  'Average Memory (mb)'
]]
pl.display(keep_columns)
"""],
["Memory Usage of Processes",
"""
#
# Get the Virtual memory usage and average memory for all processes in the k8s cluster.
#
t1 = pl.DataFrame(table='process_stats', start_time='-30s')
bytes_per_mb = 1024.0 * 1024.0
t1.vsize_mb = t1.vsize_bytes / bytes_per_mb
t1.rss_bytes_mb = t1.rss_bytes / bytes_per_mb
t1.timestamp = pl.bin(t1.time_, pl.seconds(10))
t1.cmdline = pl.upid_to_cmdline(t1.upid)

aggop = t1.groupby(['upid', 'timestamp', 'cmdline']).agg(
  vsize_mb=('vsize_mb', pl.mean),
  rss_bytes_mb=('rss_bytes_mb', pl.mean),
)
# Format column names.
aggop.pid = pl.upid_to_pid(aggop.upid)
aggop.asid = pl.upid_to_asid(aggop.upid)
aggop['Process Name'] = aggop.cmdline
# uncomment and replace number to filter on a specific pid.
#aggop = aggop[aggop.pid == 4862]
aggop['Virtual Memory (mb)'] = aggop.vsize_mb
aggop['Average Memory (mb)'] = aggop.rss_bytes_mb
keep_columns = aggop[[
  'pid',
  'Process Name',
  'asid',
  'timestamp',
  'Virtual Memory (mb)',
  'Average Memory (mb)'
]]
pl.display(keep_columns)
"""
],
["Table Schemas",
"""
#
# Get the schema for all the tables available in the system.
#
pl.display(pl.GetSchemas())
"""]

]
