queries = [
["HTTP Requests Statistics per Service",
"""
t1 = dataframe(table='http_events').range(start='-30s')

t1['service'] = t1.attr['service']
t1['http_resp_latency_ms'] = t1['http_resp_latency_ns'] / 1.0E6
t1['failure'] = t1['http_resp_status'] >= 400
t1['range_group'] = t1['time_'] - pl.modulo(t1['time_'], 1000000000)

quantiles_agg = t1.agg(by=lambda r: [r.attr.service], fn=lambda r: {
  'latency_quantiles': pl.quantiles(r.http_resp_latency_ms),
  'errors': pl.mean(r.failure),
  'throughput_total': pl.count(r.http_resp_status),
})

quantiles_agg['latency_p50'] = pl.pluck(quantiles_agg['latency_quantiles'], 'p50')
quantiles_agg['latency_p90'] = pl.pluck(quantiles_agg['latency_quantiles'], 'p90')
quantiles_agg['latency_p99'] = pl.pluck(quantiles_agg['latency_quantiles'], 'p99')
quantiles_agg['service'] = quantiles_agg.attr['service']
quantiles_table = quantiles_agg[['service', 'latency_p50', 'latency_p90', 'latency_p99', 'errors', 'throughput_total']]

# The Range aggregate to calcualte the requests per second.
range_agg = t1.agg(by=lambda r: [r.service, r.range_group], fn=lambda r: {
  'requests_per_window': pl.count(r.http_resp_status)
})

rps_table = range_agg.agg(by=lambda r: r.service, fn= lambda r: {'rps': pl.mean(r.requests_per_window)})
joined_table = quantiles_table.merge(rps_table, 
                                     how='inner', 
                                     left_on=['service'], 
                                     right_on=['service'],
                                     suffixes=['', '_x'])

joined_table['latency(p50)'] = joined_table['latency_p50']
joined_table['latency(p90)'] = joined_table['latency_p90']
joined_table['latency(p99)'] = joined_table['latency_p99']
joined_table['throughput (rps)'] = joined_table['rps']
joined_table['throughput total'] = joined_table['throughput_total']

joined_table = joined_table[[
  'service',
  'latency(p50)',
  'latency(p90)',
  'latency(p99)',
  'errors',
  'throughput (rps)',
  'throughput total']]
joined_table[joined_table['service'] != ''].result(name='out')
"""
],
["Pod Resource Usage In Time Interval",
"""
# 
# Returns the Resource Usage for each pod during an interval.
# 
t1 = dataframe(table='process_stats').range(start='-60s')

# Convert to better units.
t1['cpu_utime_s'] = t1['cpu_utime_ns'] / 1.0E9
t1['cpu_ktime_s'] = t1['cpu_ktime_ns'] / 1.0E9
t1['vsize_mb'] = t1['vsize_bytes'] / 1024.0 / 1024.0
t1['rss_bytes_mb'] = t1['rss_bytes'] / 1024.0 / 1024.0
t1['read_bytes_mb'] = t1['read_bytes'] / 1024.0 / 1024.0
t1['write_bytes_mb'] = t1['write_bytes'] / 1024.0 / 1024.0
t1['rchar_bytes_mb'] = t1['rchar_bytes'] / 1024.0 / 1024.0
t1['wchar_bytes_mb'] = t1['wchar_bytes'] / 1024.0 / 1024.0

upid_aggop = t1.agg(by=lambda r: r.upid, fn=lambda r:{
  'vsize_mb': pl.mean(r.vsize_mb),
  'rss_bytes_mb': pl.mean(r.rss_bytes_mb),
  # The following columns are all counters, so we diff the value at the beginning 
  # of the period with the value at the end of the period.
  'cpu_utime_s_end': pl.max(r.cpu_utime_s),
  'cpu_ktime_s_end': pl.max(r.cpu_ktime_s),
  'cpu_utime_s_start': pl.min(r.cpu_utime_s),
  'cpu_ktime_s_start': pl.min(r.cpu_ktime_s),
  'read_bytes_mb_end': pl.max(r.read_bytes_mb),
  'read_bytes_mb_start': pl.min(r.read_bytes_mb),
  'write_bytes_mb_end': pl.max(r.write_bytes_mb),
  'write_bytes_mb_start': pl.min(r.write_bytes_mb),
  "rchar_bytes_mb_end": pl.max(r.rchar_bytes_mb),
  "rchar_bytes_mb_start": pl.min(r.rchar_bytes_mb),
  "wchar_bytes_mb_end": pl.max(r.wchar_bytes_mb),
  "wchar_bytes_mb_start": pl.min(r.wchar_bytes_mb)
})

# The cpu time diff.
upid_aggop['cpu_utime_s'] = upid_aggop['cpu_utime_s_end'] - upid_aggop['cpu_utime_s_start']
upid_aggop['cpu_ktime_s'] = upid_aggop['cpu_ktime_s_end'] - upid_aggop['cpu_ktime_s_start']
upid_aggop['read_bytes_mb'] = upid_aggop['read_bytes_mb_end'] - upid_aggop['read_bytes_mb_start']
upid_aggop['write_bytes_mb'] = upid_aggop['write_bytes_mb_end'] - upid_aggop['write_bytes_mb_start']
upid_aggop['rchar_bytes_mb'] = upid_aggop['rchar_bytes_mb_end'] - upid_aggop['rchar_bytes_mb_start']
upid_aggop['wchar_bytes_mb'] = upid_aggop['wchar_bytes_mb_end'] - upid_aggop['wchar_bytes_mb_start']

# For this aggregate, we sum up the values as we've already calculated the average/usage 
# for the upids already.
pod_aggop = upid_aggop.agg(by=lambda r: r.attr.pod, fn=lambda r:{
  'cpu_utime_s': pl.sum(r.cpu_utime_s),
  'cpu_ktime_s': pl.sum(r.cpu_ktime_s),
  'vsize_mb': pl.sum(r.vsize_mb),
  'rss_bytes_mb': pl.sum(r.rss_bytes_mb),
  'read_bytes_mb': pl.sum(r.read_bytes_mb),
  'write_bytes_mb': pl.sum(r.write_bytes_mb),
  "rchar_bytes_mb": pl.sum(r.rchar_bytes_mb),
  "wchar_bytes_mb": pl.sum(r.wchar_bytes_mb)
})

# Format everything nicely.
pod_aggop['pod_name'] = pod_aggop.attr['pod']
pod_aggop['status'] = pl.pod_name_to_status(pod_aggop['pod_name'])
pod_aggop['Created on'] = pl.pod_name_to_start_time(pod_aggop['pod_name'])
pod_aggop['CPU User time (s)'] = pod_aggop['cpu_utime_s']
pod_aggop['CPU System time (s)'] = pod_aggop['cpu_ktime_s']
pod_aggop['Virtual Memory (mb)'] = pod_aggop['vsize_mb']
pod_aggop['Average Memory (mb)'] = pod_aggop['rss_bytes_mb']
pod_aggop['Read to IO (mb)'] = pod_aggop['read_bytes_mb']
pod_aggop['Write to IO (mb)'] = pod_aggop['write_bytes_mb']
pod_aggop['Characters Read (mb)'] = pod_aggop['rchar_bytes_mb']
pod_aggop['Characters written (mb)'] = pod_aggop['wchar_bytes_mb']

keep_columns = pod_aggop[['pod_name', 'status', 'Created on', 'CPU User time (s)', 'CPU System time (s)',
                          'Virtual Memory (mb)', 'Average Memory (mb)', 'Read to IO (mb)', 'Write to IO (mb)',
                          'Characters Read (mb)', 'Characters written (mb)']]
keep_columns.result(name='out')
"""],
["Total Resource Usage During Pod Lifetime",
"""
# 
# Returns the Total Resource Usage for each pod.
# 
t1 = dataframe(table='process_stats').range(start='-60s')

# Convert to better units.
t1['cpu_utime_s'] = t1['cpu_utime_ns'] / 1.0E9
t1['cpu_ktime_s'] = t1['cpu_ktime_ns'] / 1.0E9
t1['vsize_mb'] = t1['vsize_bytes'] / 1024.0 / 1024.0
t1['rss_bytes_mb'] = t1['rss_bytes'] / 1024.0 / 1024.0
t1['read_bytes_mb'] = t1['read_bytes'] / 1024.0 / 1024.0
t1['write_bytes_mb'] = t1['write_bytes'] / 1024.0 / 1024.0
t1['rchar_bytes_mb'] = t1['rchar_bytes'] / 1024.0 / 1024.0
t1['wchar_bytes_mb'] = t1['wchar_bytes'] / 1024.0 / 1024.0

upid_aggop = t1.agg(by=lambda r: r.upid, fn=lambda r:{
  'vsize_mb': pl.mean(r.vsize_mb),
  'rss_bytes_mb': pl.mean(r.rss_bytes_mb),
  # The following columns are all counters, so we take the maximum value.
  'cpu_utime_s': pl.max(r.cpu_utime_s),
  'cpu_ktime_s': pl.max(r.cpu_ktime_s),
  'read_bytes_mb': pl.max(r.read_bytes_mb),
  'write_bytes_mb': pl.max(r.write_bytes_mb),
  "rchar_bytes_mb": pl.max(r.rchar_bytes_mb),
  "wchar_bytes_mb": pl.max(r.wchar_bytes_mb),
})
# For this aggregate, we sum up the values as we've already calculated the average/usage 
# for the upids already.
pod_aggop = upid_aggop.agg(by=lambda r: r.attr.pod, fn=lambda r:{
  'cpu_utime_s': pl.sum(r.cpu_utime_s),
  'cpu_ktime_s': pl.sum(r.cpu_ktime_s),
  'vsize_mb': pl.sum(r.vsize_mb),
  'rss_bytes_mb': pl.sum(r.rss_bytes_mb),
  'read_bytes_mb': pl.sum(r.read_bytes_mb),
  'write_bytes_mb': pl.sum(r.write_bytes_mb),
  "rchar_bytes_mb": pl.sum(r.rchar_bytes_mb),
  "wchar_bytes_mb": pl.sum(r.wchar_bytes_mb)
})

# Format everything nicely.
pod_aggop['pod_name'] = pod_aggop.attr['pod']
pod_aggop['status'] = pl.pod_name_to_status(pod_aggop['pod_name'])
pod_aggop['Created on'] = pl.pod_name_to_start_time(pod_aggop['pod_name'])
pod_aggop['CPU User time (s)'] = pod_aggop['cpu_utime_s']
pod_aggop['CPU System time (s)'] = pod_aggop['cpu_ktime_s']
pod_aggop['Virtual Memory (mb)'] = pod_aggop['vsize_mb']
pod_aggop['Average Memory (mb)'] = pod_aggop['rss_bytes_mb']
pod_aggop['Read to IO (mb)'] = pod_aggop['read_bytes_mb']
pod_aggop['Write to IO (mb)'] = pod_aggop['write_bytes_mb']
pod_aggop['Characters Read (mb)'] = pod_aggop['rchar_bytes_mb']
pod_aggop['Characters written (mb)'] = pod_aggop['wchar_bytes_mb']

keep_columns = pod_aggop[['pod_name', 'status', 'Created on', 'CPU User time (s)', 'CPU System time (s)',
                          'Virtual Memory (mb)', 'Average Memory (mb)', 'Read to IO (mb)', 'Write to IO (mb)',
                          'Characters Read (mb)', 'Characters written (mb)']]
keep_columns.result(name='out')
"""],
["Sample HTTP Data",
"""
t1 = dataframe(table='http_events', select=['time_', 'remote_addr', 'remote_port', 'http_resp_status', 'http_resp_message', 'http_resp_body', 'http_resp_latency_ns']).range(start='-30s')
t1['http_resp_latency_ms'] = t1['http_resp_latency_ns'] / 1.0E6
t2 = t1.drop(columns=['http_resp_latency_ns']).limit(rows=100).result(name='resp_table')
"""],
["Sample MySQL Data",
"""
t1 = dataframe(table='mysql_events', select=['time_', 'remote_addr', 'remote_port', 'req_cmd', 'req_body', 'resp_status', 'resp_body', 'latency_ns']).range(start='-30s')
t1['latency_ms'] = t1['latency_ns'] / 1.0E6
t2 = t1.drop(columns=['latency_ns']).limit(rows=100).result(name='resp_table')
"""],
["Network Stats",
"""
t1 = dataframe(table='network_stats', select=['time_', 'pod_id', 'rx_bytes', 'rx_packets', 'rx_errors',
                                         'rx_drops', 'tx_bytes', 'tx_packets', 'tx_errors', 'tx_drops']).range(start='-30s')
t2 = t1.limit(rows=100).result(name='stats')
"""]
]
